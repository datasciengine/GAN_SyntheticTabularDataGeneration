{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5ce26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score,\\\n",
    "                            accuracy_score, balanced_accuracy_score,classification_report,\\\n",
    "                            plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.utils import shuffle\n",
    "import keras\n",
    "\n",
    "np.random.seed(1635848)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703d5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575b997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.Model):\n",
    "    def __init__(self, latent_dim=32, out_shape=14, num_classes=2):\n",
    "        super(Generator, self).__init__(name=\"generator\")\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.out_shape = out_shape \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.dense_in = Dense(128, use_bias=False, input_dim=self.latent_dim, name=\"Dense1\")\n",
    "        self.dense_out = Dense(self.out_shape, activation='tanh')\n",
    "        self.dense1 = Dense(256)\n",
    "        self.dense2 = Dense(512)\n",
    "        self.dropout02 = Dropout(0.2)\n",
    "        self.bn1 = BatchNormalization(momentum=0.4)\n",
    "        self.bn2 = BatchNormalization(momentum=0.8)\n",
    "        self.leaky_relu01 = LeakyReLU(alpha=0.1)\n",
    "        \n",
    "        \n",
    "    def call(self, model_input):\n",
    "        x = self.dense_in(model_input)\n",
    "        x = self.dropout02(x)\n",
    "        x = self.leaky_relu01(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout02(x)\n",
    "        x = self.leaky_relu01(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout02(x)\n",
    "        x = self.leaky_relu01(x)\n",
    "        gen_sample = self.dense_out(x)\n",
    "        return gen_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2c21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(keras.Model):\n",
    "    def __init__(self, out_shape=14, num_classes=2):\n",
    "        super(Discriminator, self).__init__(name=\"discriminator\")\n",
    "        \n",
    "        self.out_shape = out_shape \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "        self.dense_in = Dense(512, input_dim=self.out_shape, kernel_initializer=self.init)\n",
    "        self.dense_out = Dense(1, activation='sigmoid')\n",
    "        self.leaky_relu02 = LeakyReLU(alpha=0.2)\n",
    "        self.dropout04 = Dropout(0.4)\n",
    "        self.dense1 = Dense(256, kernel_initializer=self.init)\n",
    "        self.dense2 = Dense(128, kernel_initializer=self.init)\n",
    "        \n",
    "    def call(self, model_input):\n",
    "        x = self.dense_in(model_input)\n",
    "        x = self.leaky_relu02(x)\n",
    "        \n",
    "        x = self.dense1(x)\n",
    "        x = self.leaky_relu02(x)\n",
    "        x = self.dropout04(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        x = self.leaky_relu02(x)\n",
    "        x = self.dropout04(x)\n",
    "        \n",
    "        validity = self.dense_out(x)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61931ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cGAN():\n",
    "    \n",
    "    def __init__(self, latent_dim=32, out_shape=14, num_classes=2):\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.out_shape = out_shape \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # creating discriminator and generator objects\n",
    "        self.discriminator_obj = Discriminator(out_shape=self.out_shape, num_classes=self.num_classes)\n",
    "        self.generator_obj = Generator(latent_dim=self.latent_dim, out_shape=self.out_shape, num_classes=self.num_classes)\n",
    "        \n",
    "        # using Adam as our optimizer\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        \n",
    "        # building the discriminator\n",
    "        self.discriminator = self.discriminator_obj.create_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # building the generator\n",
    "        self.generator = self.generator_obj.create_generator()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        gen_samples = self.generator([noise, label])\n",
    "        \n",
    "        # we don't train discriminator when training generator\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([gen_samples, label])\n",
    "\n",
    "        # combining both models\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "                              optimizer=optimizer,\n",
    "                             metrics=['accuracy'])\n",
    "        \n",
    "    def train(self, X_train, y_train, pos_index, neg_index, epochs, sampling=False, batch_size=32, sample_interval=100, plot=True): \n",
    "        \n",
    "        # though not recommended, defining losses as global helps as in analysing our cgan out of the class\n",
    "        global G_losses\n",
    "        global D_losses\n",
    "        \n",
    "        G_losses = []\n",
    "        D_losses = []\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # if sampling==True --> train discriminator with 8 sample from postivite class and rest with negative class\n",
    "            if sampling:\n",
    "                idx1 = np.random.choice(pos_index, 8)\n",
    "                idx0 = np.random.choice(neg_index, batch_size-8)\n",
    "                idx = np.concatenate((idx1, idx0))\n",
    "            # if sampling!=True --> train discriminator using random instances in batches of 32\n",
    "            else:\n",
    "                idx = np.random.choice(len(y_train), batch_size)\n",
    "            samples, labels = X_train[idx], y_train[idx]\n",
    "            samples, labels = shuffle(samples, labels)\n",
    "            \n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            #print(noise)\n",
    "            #print(labels)\n",
    "            gen_samples = self.generator.predict([noise, labels])\n",
    "\n",
    "            # label smoothing\n",
    "            if epoch < epochs//1.5:\n",
    "                valid_smooth = (valid+0.1)-(np.random.random(valid.shape)*0.1)\n",
    "                fake_smooth = (fake-0.1)+(np.random.random(fake.shape)*0.1)\n",
    "            else:\n",
    "                valid_smooth = valid \n",
    "                fake_smooth = fake\n",
    "                \n",
    "            # Train the discriminator\n",
    "            self.discriminator.trainable = True\n",
    "            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train Generator\n",
    "            self.discriminator.trainable = False\n",
    "            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "            if (epoch+1)%sample_interval==0:\n",
    "                print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "                  % (epoch, epochs, d_loss[0], g_loss[0]))\n",
    "            G_losses.append(g_loss[0])\n",
    "            D_losses.append(d_loss[0])\n",
    "            if plot:\n",
    "                if epoch+1==epochs:\n",
    "                    plt.figure(figsize=(10,5))\n",
    "                    plt.title(\"Generator and Discriminator Loss\")\n",
    "                    plt.plot(G_losses,label=\"G\")\n",
    "                    plt.plot(D_losses,label=\"D\")\n",
    "                    plt.xlabel(\"iterations\")\n",
    "                    plt.ylabel(\"Loss\")\n",
    "                    plt.legend()\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27983e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "num_classes = 2\n",
    "out_shape = 14\n",
    "\n",
    "noise = Input(shape=(latent_dim,))\n",
    "label = Input(shape=(1,), dtype='int32')\n",
    "label_embedding = Flatten()(Embedding(num_classes, latent_dim)(label))\n",
    "generator_input = multiply([noise, label_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim=latent_dim, out_shape=out_shape, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47184fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_samples = generator(generator_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c586f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa6656",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sample = Input(shape=(out_shape,))\n",
    "label = Input(shape=(1,), dtype='int32')\n",
    "label_embedding = Flatten()(Embedding(num_classes, out_shape)(label))\n",
    "discriminator_input = multiply([gen_samples, label_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38749815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(out_shape=out_shape, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b269403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "valid = discriminator(discriminator_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a33777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cGAN():\n",
    "    \n",
    "    def __init__(self, latent_dim=32, out_shape=14, num_classes=2):\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.out_shape = out_shape \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # creating discriminator and generator objects\n",
    "        self.discriminator = Discriminator(out_shape=out_shape, num_classes=num_classes)\n",
    "        self.generator = Generator(latent_dim=latent_dim, out_shape=out_shape, num_classes=num_classes)\n",
    "        \n",
    "        # using Adam as our optimizer\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        \n",
    "        # building the discriminator\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # building the generator\n",
    "        noise = Input(shape=(latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(num_classes, latent_dim)(label))\n",
    "        generator_input = multiply([noise, label_embedding])\n",
    "        gen_samples = self.generator(generator_input)\n",
    "        \n",
    "        # we don't train discriminator when training generator\n",
    "        self.discriminator.trainable = False\n",
    "        gen_sample = Input(shape=(out_shape,))\n",
    "        \n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(num_classes, out_shape)(label))\n",
    "        discriminator_input = multiply([gen_samples, label_embedding])\n",
    "        \n",
    "        valid = self.discriminator(discriminator_input)\n",
    "\n",
    "        # combining both models\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "                              optimizer=optimizer,\n",
    "                             metrics=['accuracy'])\n",
    "        \n",
    "    def train(self, X_train, y_train, pos_index, neg_index, epochs, sampling=False, batch_size=32, sample_interval=100, plot=True): \n",
    "        \n",
    "        # though not recommended, defining losses as global helps as in analysing our cgan out of the class\n",
    "        global G_losses\n",
    "        global D_losses\n",
    "        \n",
    "        G_losses = []\n",
    "        D_losses = []\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # if sampling==True --> train discriminator with 8 sample from postivite class and rest with negative class\n",
    "            if sampling:\n",
    "                idx1 = np.random.choice(pos_index, 8)\n",
    "                idx0 = np.random.choice(neg_index, batch_size-8)\n",
    "                idx = np.concatenate((idx1, idx0))\n",
    "            # if sampling!=True --> train discriminator using random instances in batches of 32\n",
    "            else:\n",
    "                idx = np.random.choice(len(y_train), batch_size)\n",
    "            samples, labels = X_train[idx], y_train[idx]\n",
    "            samples, labels = shuffle(samples, labels)\n",
    "            \n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            #print(noise)\n",
    "            #print(labels)\n",
    "            gen_samples = self.generator.predict([noise, labels])\n",
    "\n",
    "            # label smoothing\n",
    "            if epoch < epochs//1.5:\n",
    "                valid_smooth = (valid+0.1)-(np.random.random(valid.shape)*0.1)\n",
    "                fake_smooth = (fake-0.1)+(np.random.random(fake.shape)*0.1)\n",
    "            else:\n",
    "                valid_smooth = valid \n",
    "                fake_smooth = fake\n",
    "                \n",
    "            # Train the discriminator\n",
    "            self.discriminator.trainable = True\n",
    "            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train Generator\n",
    "            self.discriminator.trainable = False\n",
    "            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "            if (epoch+1)%sample_interval==0:\n",
    "                print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "                  % (epoch, epochs, d_loss[0], g_loss[0]))\n",
    "            G_losses.append(g_loss[0])\n",
    "            D_losses.append(d_loss[0])\n",
    "            if plot:\n",
    "                if epoch+1==epochs:\n",
    "                    plt.figure(figsize=(10,5))\n",
    "                    plt.title(\"Generator and Discriminator Loss\")\n",
    "                    plt.plot(G_losses,label=\"G\")\n",
    "                    plt.plot(D_losses,label=\"D\")\n",
    "                    plt.xlabel(\"iterations\")\n",
    "                    plt.ylabel(\"Loss\")\n",
    "                    plt.legend()\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae2a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6fabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "num_classes = 2\n",
    "out_shape = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d5d6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = latent_dim\n",
    "out_shape = out_shape \n",
    "num_classes = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019a0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Adam as our optimizer\n",
    "optimizer = keras.optimizer_v2.adam.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f9de94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# building the discriminator\n",
    "discriminator = Discriminator(out_shape=out_shape, num_classes=num_classes)\n",
    "discriminator.compile(loss=['binary_crossentropy'],\n",
    "                            optimizer=optimizer,\n",
    "                            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6b74684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Discriminator at 0x23087716850>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a2b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd21b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the generator\n",
    "generator = Generator(latent_dim=latent_dim, out_shape=out_shape, num_classes=num_classes)\n",
    "\n",
    "noise = Input(shape=(latent_dim,))\n",
    "label = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "label_embedding = Flatten()(Embedding(num_classes, latent_dim)(label))\n",
    "generator_input = multiply([noise, label_embedding])\n",
    "gen_samples = generator(generator_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1b5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81da583d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 14) dtype=float32 (created by layer 'input_3')>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0af9a260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 14) dtype=float32 (created by layer 'dense_4')>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8c67f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't train discriminator when training generator\n",
    "discriminator.trainable = False\n",
    "gen_sample = Input(shape=(out_shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "521bcf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = Input(shape=(1,), dtype='int32')\n",
    "label_embedding = Flatten()(Embedding(num_classes, out_shape)(label))\n",
    "discriminator_input = multiply([gen_samples, label_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a7c2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = discriminator(discriminator_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27c0448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cGAN(keras.Model):\n",
    "    def __inti__(self, latent_dim=32, out_shape=14, num_classes=2):\n",
    "        #super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.out_shape = out_shape \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        optimizer = keras.optimizer_v2.adam.Adam()\n",
    "        self.discriminator = Discriminator(out_shape=out_shape, num_classes=num_classes)\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "                                    optimizer=optimizer,\n",
    "                                    metrics=['accuracy'])\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        self.generator = Generator(latent_dim=latent_dim, out_shape=out_shape, num_classes=num_classes)\n",
    "    \n",
    "    def call(self):\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "        generator_input = multiply([noise, label_embedding])\n",
    "        gen_samples = self.generator(generator_input)\n",
    "        label_embedding2 = Flatten()(Embedding(self.num_classes, self.out_shape)(label))\n",
    "        discriminator_input = multiply([gen_samples, label_embedding2])\n",
    "        valid = self.discriminator(discriminator_input)\n",
    "        return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "262c7c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan = cGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6346644f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KALYBE~1\\AppData\\Local\\Temp/ipykernel_8920/475127742.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcgan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu_tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;31m#   not to any other argument.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;31m# - setting the SavedModel saving spec.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split_out_first_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m     \u001b[0minput_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu_tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_split_out_first_arg\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2947\u001b[0m       \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2948\u001b[0m       \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2949\u001b[1;33m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fn_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2950\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2951\u001b[0m       \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fn_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "prob = cgan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c344c815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2a8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffcd359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a16e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the discriminator\n",
    "self.discriminator = self.discriminator()\n",
    "self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "# building the generator\n",
    "self.generator = self.generator()\n",
    "\n",
    "noise = Input(shape=(self.latent_dim,))\n",
    "label = Input(shape=(1,))\n",
    "gen_samples = self.generator([noise, label])\n",
    "\n",
    "# we don't train discriminator when training generator\n",
    "self.discriminator.trainable = False\n",
    "valid = self.discriminator([gen_samples, label])\n",
    "\n",
    "# combining both models\n",
    "self.combined = Model([noise, label], valid)\n",
    "self.combined.compile(loss=['binary_crossentropy'],\n",
    "                      optimizer=optimizer,\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf9f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af2b38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T18:57:56.242129Z",
     "iopub.status.busy": "2021-03-14T18:57:56.241459Z",
     "iopub.status.idle": "2021-03-14T18:57:56.501901Z",
     "shell.execute_reply": "2021-03-14T18:57:56.501357Z"
    },
    "papermill": {
     "duration": 0.292671,
     "end_time": "2021-03-14T18:57:56.502049",
     "exception": false,
     "start_time": "2021-03-14T18:57:56.209378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('adult.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ff20b",
   "metadata": {
    "papermill": {
     "duration": 0.02821,
     "end_time": "2021-03-14T18:57:56.558913",
     "exception": false,
     "start_time": "2021-03-14T18:57:56.530703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Before employing any algorithms, we will first preprocess some data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5195e5",
   "metadata": {
    "papermill": {
     "duration": 0.028048,
     "end_time": "2021-03-14T18:57:56.615301",
     "exception": false,
     "start_time": "2021-03-14T18:57:56.587253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9138906",
   "metadata": {
    "papermill": {
     "duration": 0.028486,
     "end_time": "2021-03-14T18:57:56.672891",
     "exception": false,
     "start_time": "2021-03-14T18:57:56.644405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since the goal of this notebook is to examine how good the generated synthetic data is, we won't analyse and do any feature engineering. It is also not that important that we get the best possible result with the algorithm, so that's one of the reasons why we will only use label-encoding (on some features normally one-hot encoding should be a better approach). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ded70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T18:57:56.750226Z",
     "iopub.status.busy": "2021-03-14T18:57:56.749277Z",
     "iopub.status.idle": "2021-03-14T18:57:56.861402Z",
     "shell.execute_reply": "2021-03-14T18:57:56.860853Z"
    },
    "papermill": {
     "duration": 0.160158,
     "end_time": "2021-03-14T18:57:56.861551",
     "exception": false,
     "start_time": "2021-03-14T18:57:56.701393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "for i in ['workclass','education','marital.status','occupation','relationship','race','sex','native.country','income']:\n",
    "    df[i] = le.fit_transform(df[i].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefa149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T18:57:56.934314Z",
     "iopub.status.busy": "2021-03-14T18:57:56.933355Z",
     "iopub.status.idle": "2021-03-14T18:57:56.938317Z",
     "shell.execute_reply": "2021-03-14T18:57:56.937677Z"
    },
    "papermill": {
     "duration": 0.048424,
     "end_time": "2021-03-14T18:57:56.938456",
     "exception": false,
     "start_time": "2021-03-14T18:57:56.890032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9970f86b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T18:57:57.004415Z",
     "iopub.status.busy": "2021-03-14T18:57:57.003450Z",
     "iopub.status.idle": "2021-03-14T18:57:57.010683Z",
     "shell.execute_reply": "2021-03-14T18:57:57.011167Z"
    },
    "papermill": {
     "duration": 0.043669,
     "end_time": "2021-03-14T18:57:57.011424",
     "exception": false,
     "start_time": "2021-03-14T18:57:56.967755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.income.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc8140",
   "metadata": {
    "papermill": {
     "duration": 0.030588,
     "end_time": "2021-03-14T18:57:57.071712",
     "exception": false,
     "start_time": "2021-03-14T18:57:57.041124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Splitting the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254456ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T18:57:57.139557Z",
     "iopub.status.busy": "2021-03-14T18:57:57.138411Z",
     "iopub.status.idle": "2021-03-14T18:57:57.172662Z",
     "shell.execute_reply": "2021-03-14T18:57:57.172014Z"
    },
    "papermill": {
     "duration": 0.071211,
     "end_time": "2021-03-14T18:57:57.172869",
     "exception": false,
     "start_time": "2021-03-14T18:57:57.101658",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(df.drop('income', 1))\n",
    "y = df['income'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a858100",
   "metadata": {
    "papermill": {
     "duration": 0.029781,
     "end_time": "2021-03-14T18:57:57.232219",
     "exception": false,
     "start_time": "2021-03-14T18:57:57.202438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Classifying using real trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b45ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T18:57:57.298704Z",
     "iopub.status.busy": "2021-03-14T18:57:57.297712Z",
     "iopub.status.idle": "2021-03-14T18:57:57.860462Z",
     "shell.execute_reply": "2021-03-14T18:57:57.860937Z"
    },
    "papermill": {
     "duration": 0.598783,
     "end_time": "2021-03-14T18:57:57.861114",
     "exception": false,
     "start_time": "2021-03-14T18:57:57.262331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgb_1 = lgb.LGBMClassifier()\n",
    "lgb_1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lgb_1.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "plot_confusion_matrix(lgb_1, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21178722",
   "metadata": {
    "papermill": {
     "duration": 0.030765,
     "end_time": "2021-03-14T18:57:57.922986",
     "exception": false,
     "start_time": "2021-03-14T18:57:57.892221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training cGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef459a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.latent_dim = latent_dim\n",
    "self.out_shape = out_shape \n",
    "self.num_classes = num_classes\n",
    "\n",
    "# creating discriminator and generator objects\n",
    "self.discriminator = Discriminator(out_shape=out_shape, num_classes=num_classes)\n",
    "self.generator = Generator(latent_dim=latent_dim, out_shape=out_shape, num_classes=num_classes)\n",
    "\n",
    "# using Adam as our optimizer\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# building the discriminator\n",
    "\n",
    "self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "# building the generator\n",
    "\n",
    "noise = Input(shape=(latent_dim,))\n",
    "label = Input(shape=(1,), dtype='int32')\n",
    "label_embedding = Flatten()(Embedding(num_classes, latent_dim)(label))\n",
    "generator_input = multiply([noise, label_embedding])\n",
    "\n",
    "gen_samples = self.generator(generator_input)\n",
    "\n",
    "# we don't train discriminator when training generator\n",
    "self.discriminator.trainable = False\n",
    "gen_sample = Input(shape=(out_shape,))\n",
    "\n",
    "label = Input(shape=(1,), dtype='int32')\n",
    "label_embedding = Flatten()(Embedding(num_classes, out_shape)(label))\n",
    "discriminator_input = multiply([gen_samples, label_embedding])\n",
    "\n",
    "valid = self.discriminator(discriminator_input)\n",
    "\n",
    "# combining both models\n",
    "self.combined = Model([noise, label], valid)\n",
    "self.combined.compile(loss=['binary_crossentropy'],\n",
    "                      optimizer=optimizer,\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15aa0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef570e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e7879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533008d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T18:57:57.989102Z",
     "iopub.status.busy": "2021-03-14T18:57:57.988117Z",
     "iopub.status.idle": "2021-03-14T18:57:58.354084Z",
     "shell.execute_reply": "2021-03-14T18:57:58.353403Z"
    },
    "papermill": {
     "duration": 0.399933,
     "end_time": "2021-03-14T18:57:58.354226",
     "exception": false,
     "start_time": "2021-03-14T18:57:57.954293",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cgan = cGAN(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7b826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T18:57:58.422228Z",
     "iopub.status.busy": "2021-03-14T18:57:58.421562Z",
     "iopub.status.idle": "2021-03-14T18:58:33.511742Z",
     "shell.execute_reply": "2021-03-14T18:58:33.511226Z"
    },
    "papermill": {
     "duration": 35.126648,
     "end_time": "2021-03-14T18:58:33.511909",
     "exception": false,
     "start_time": "2021-03-14T18:57:58.385261",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,1)\n",
    "pos_index = np.where(y_train==1)[0]\n",
    "neg_index = np.where(y_train==0)[0]\n",
    "cgan.train(X_train, y_train, pos_index, neg_index, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b59fe",
   "metadata": {
    "papermill": {
     "duration": 0.034047,
     "end_time": "2021-03-14T18:58:33.580342",
     "exception": false,
     "start_time": "2021-03-14T18:58:33.546295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generating new instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4556e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T18:58:33.655372Z",
     "iopub.status.busy": "2021-03-14T18:58:33.654567Z",
     "iopub.status.idle": "2021-03-14T18:58:34.930155Z",
     "shell.execute_reply": "2021-03-14T18:58:34.928715Z"
    },
    "papermill": {
     "duration": 1.315571,
     "end_time": "2021-03-14T18:58:34.930353",
     "exception": false,
     "start_time": "2021-03-14T18:58:33.614782",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we want to generate 19758 instances with class value 0 since that represents how many 0s are in the label of the real training set\n",
    "noise = np.random.normal(0, 1, (len(df[df[\"income\"] == 0]), 32))\n",
    "sampled_labels = np.zeros(len(df[df[\"income\"] == 0])).reshape(-1, 1)\n",
    "\n",
    "\n",
    "gen_samples = cgan.generator.predict([noise, sampled_labels])\n",
    "\n",
    "gen_df = pd.DataFrame(data = gen_samples,\n",
    "                      columns = df.drop('income',1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852b05e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T18:58:35.006195Z",
     "iopub.status.busy": "2021-03-14T18:58:35.005439Z",
     "iopub.status.idle": "2021-03-14T18:58:35.429671Z",
     "shell.execute_reply": "2021-03-14T18:58:35.428538Z"
    },
    "papermill": {
     "duration": 0.465276,
     "end_time": "2021-03-14T18:58:35.429853",
     "exception": false,
     "start_time": "2021-03-14T18:58:34.964577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we want to generate 6290 instances with class value 1 since that represents how many 1s are in the label of the real training set\n",
    "noise_2 = np.random.normal(0, 1, (len(df[df[\"income\"] == 1]), 32))\n",
    "sampled_labels_2 = np.ones(len(df[df[\"income\"] == 1])).reshape(-1, 1)\n",
    "\n",
    "\n",
    "gen_samples_2 = cgan.generator.predict([noise_2, sampled_labels_2])\n",
    "\n",
    "gen_df_2 = pd.DataFrame(data = gen_samples_2,\n",
    "                      columns = df.drop('income',1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1265f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ea91eb5",
   "metadata": {
    "papermill": {
     "duration": 0.034359,
     "end_time": "2021-03-14T18:58:35.498785",
     "exception": false,
     "start_time": "2021-03-14T18:58:35.464426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Combining generated instances into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4c795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-14T18:58:35.575542Z",
     "iopub.status.busy": "2021-03-14T18:58:35.574674Z",
     "iopub.status.idle": "2021-03-14T18:58:35.586121Z",
     "shell.execute_reply": "2021-03-14T18:58:35.585550Z"
    },
    "papermill": {
     "duration": 0.053301,
     "end_time": "2021-03-14T18:58:35.586331",
     "exception": false,
     "start_time": "2021-03-14T18:58:35.533030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_df_2['income'] = 1\n",
    "gen_df['income']=0\n",
    "\n",
    "df_gan = pd.concat([gen_df_2, gen_df], ignore_index=True, sort=False)\n",
    "df_gan = df_gan.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_train_2 = df_gan.drop('income', 1)\n",
    "y_train_2 = df_gan['income'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256fed00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d07943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad01cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
