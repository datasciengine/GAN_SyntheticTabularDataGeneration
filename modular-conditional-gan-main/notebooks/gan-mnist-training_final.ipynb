{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular Data için Çalışan Koşullu GAN'ın Dinamik hale getirilmesi ve Fonksiyonelleştirilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "Num GPUs Available:  1\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import PIL \n",
    "import imageio\n",
    "from IPython import display\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import sys \n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from gan.networks import Generator, Discriminator, GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CROSSING</th>\n",
       "      <th>FINISSHING</th>\n",
       "      <th>HEADING_ACCURACY</th>\n",
       "      <th>SHORT_PASSING</th>\n",
       "      <th>VOLLEYS</th>\n",
       "      <th>DRIBBLING</th>\n",
       "      <th>CURVE</th>\n",
       "      <th>FREE_KICK_ACCURACY</th>\n",
       "      <th>LONG_PASSING</th>\n",
       "      <th>BALL_CONTROL</th>\n",
       "      <th>...</th>\n",
       "      <th>SPRINT_SPEED</th>\n",
       "      <th>AGILITY</th>\n",
       "      <th>REACTION</th>\n",
       "      <th>BALANCE</th>\n",
       "      <th>SHOT_POWER</th>\n",
       "      <th>JUMPING</th>\n",
       "      <th>STAMINA</th>\n",
       "      <th>STRENGTH</th>\n",
       "      <th>LONG_SHOTS</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CROSSING  FINISSHING  HEADING_ACCURACY  SHORT_PASSING  VOLLEYS  DRIBBLING  \\\n",
       "0      56.0        28.0              54.0           68.0     55.0       58.0   \n",
       "1      21.0        20.0              26.0           27.0     21.0       30.0   \n",
       "2      49.0        67.0              47.0           73.0     59.0       71.0   \n",
       "3      41.0        62.0              60.0           60.0     52.0       70.0   \n",
       "4      60.0        41.0              46.0           70.0     60.0       62.0   \n",
       "\n",
       "   CURVE  FREE_KICK_ACCURACY  LONG_PASSING  BALL_CONTROL  ...  SPRINT_SPEED  \\\n",
       "0   58.0                49.0          65.0          62.0  ...          56.0   \n",
       "1    8.0                11.0          75.0          34.0  ...          56.0   \n",
       "2   63.0                62.0          67.0          72.0  ...          72.0   \n",
       "3   49.0                38.0          48.0          68.0  ...          69.0   \n",
       "4   47.0                31.0          68.0          67.0  ...          65.0   \n",
       "\n",
       "   AGILITY  REACTION  BALANCE  SHOT_POWER  JUMPING  STAMINA  STRENGTH  \\\n",
       "0     65.0      56.0     66.0        58.0     55.0     65.0      55.0   \n",
       "1     60.0      75.0     56.0        26.0     72.0     72.0      71.0   \n",
       "2     75.0      69.0     72.0        67.0     68.0     81.0      58.0   \n",
       "3     74.0      59.0     71.0        59.0     62.0     57.0      60.0   \n",
       "4     72.0      68.0     71.0        42.0     72.0     75.0      63.0   \n",
       "\n",
       "   LONG_SHOTS    y  \n",
       "0        61.0  0.0  \n",
       "1        20.0  0.0  \n",
       "2        62.0  0.0  \n",
       "3        51.0  0.0  \n",
       "4        47.0  0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"20ft_12k.csv\"\n",
    "data_path = \"C:/Users/kalybeai-dxlc693/Desktop/GANS/modular-conditional-gan-main/datasets/input_reals/{0}\".format(file_name)\n",
    "data = pd.read_csv(data_path, sep=\";\")\n",
    "\n",
    "data.loc[data[data[\"CROSSING\"] >= data[\"CROSSING\"].median()].index, \"y\"] = 1\n",
    "data.loc[data[data[\"CROSSING\"] < data[\"CROSSING\"].median()].index, \"y\"] = 0\n",
    "\n",
    "df = data.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KALYBE~1\\AppData\\Local\\Temp/ipykernel_8308/3217749644.py:12: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X_train = scaler.fit_transform(df.drop(condition_feature, 1))\n"
     ]
    }
   ],
   "source": [
    "# data_path = \"C:/Users/kalybeai-dxlc693/Desktop/GANS/modular-conditional-gan-main/datasets/input_reals/10ft_3k.csv\"\n",
    "# data = pd.read_csv(data_path, sep=\";\")\n",
    "# df = data.copy()\n",
    "\n",
    "# # preprocess\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# for i in ['workclass','education','marital.status','occupation','relationship','race','sex','native.country','income']:\n",
    "#     df[i] = le.fit_transform(df[i].astype(str))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "condition_feature = \"y\"\n",
    "X_train = scaler.fit_transform(df.drop(condition_feature, 1))\n",
    "y_train = df[condition_feature].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 32\n",
    "latent_dim = 100\n",
    "out_shape = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss & optimizer\n",
    "def loss_fn(labels, output):\n",
    "    return keras.losses.BinaryCrossentropy(from_logits=True)(labels, output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.05)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5861, 20)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalybeai-dxlc693\\anaconda3\\envs\\gpu_tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:5016: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 3s 6ms/step - d_loss: 0.6197 - g_loss: 0.7340\n",
      "Epoch 2/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.6208 - g_loss: 0.8890\n",
      "Epoch 3/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.5961 - g_loss: 1.0174\n",
      "Epoch 4/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.5519 - g_loss: 1.1597\n",
      "Epoch 5/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.5772 - g_loss: 1.1170\n",
      "Epoch 6/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.5960 - g_loss: 1.0190\n",
      "Epoch 7/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.5890 - g_loss: 0.9621\n",
      "Epoch 8/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.5927 - g_loss: 0.9436\n",
      "Epoch 9/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.5830 - g_loss: 0.9369\n",
      "Epoch 10/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.5796 - g_loss: 0.9627\n",
      "Epoch 11/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.5758 - g_loss: 0.9844\n",
      "Epoch 12/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.5699 - g_loss: 0.9886\n",
      "Epoch 13/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.5632 - g_loss: 1.0103\n",
      "Epoch 14/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.5601 - g_loss: 1.0059: 0s\n",
      "Epoch 15/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.5548 - g_loss: 1.0336\n",
      "Epoch 16/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.5471 - g_loss: 1.0623\n",
      "Epoch 17/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.5376 - g_loss: 1.0770\n",
      "Epoch 18/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.5271 - g_loss: 1.0979\n",
      "Epoch 19/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.5248 - g_loss: 1.1240\n",
      "Epoch 20/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.5169 - g_loss: 1.1461\n",
      "Epoch 21/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.5088 - g_loss: 1.1674\n",
      "Epoch 22/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.5050 - g_loss: 1.1739\n",
      "Epoch 23/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4965 - g_loss: 1.1948\n",
      "Epoch 24/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4911 - g_loss: 1.2151\n",
      "Epoch 25/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.4872 - g_loss: 1.2333\n",
      "Epoch 26/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.4837 - g_loss: 1.2510\n",
      "Epoch 27/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.4723 - g_loss: 1.2560\n",
      "Epoch 28/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4696 - g_loss: 1.3096\n",
      "Epoch 29/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4681 - g_loss: 1.2981\n",
      "Epoch 30/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4631 - g_loss: 1.3196\n",
      "Epoch 31/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4561 - g_loss: 1.3341\n",
      "Epoch 32/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4528 - g_loss: 1.3571\n",
      "Epoch 33/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4449 - g_loss: 1.3812\n",
      "Epoch 34/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4423 - g_loss: 1.3841\n",
      "Epoch 35/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4382 - g_loss: 1.4018\n",
      "Epoch 36/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4285 - g_loss: 1.4213\n",
      "Epoch 37/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4273 - g_loss: 1.4554\n",
      "Epoch 38/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4230 - g_loss: 1.4568\n",
      "Epoch 39/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4185 - g_loss: 1.4813\n",
      "Epoch 40/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4125 - g_loss: 1.4717\n",
      "Epoch 41/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4116 - g_loss: 1.5156\n",
      "Epoch 42/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4084 - g_loss: 1.5245\n",
      "Epoch 43/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4048 - g_loss: 1.5263\n",
      "Epoch 44/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.4015 - g_loss: 1.5386\n",
      "Epoch 45/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.3963 - g_loss: 1.5616\n",
      "Epoch 46/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.3955 - g_loss: 1.5529\n",
      "Epoch 47/50\n",
      "184/184 [==============================] - 1s 6ms/step - d_loss: 0.3938 - g_loss: 1.5918\n",
      "Epoch 48/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.3909 - g_loss: 1.5974\n",
      "Epoch 49/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.3847 - g_loss: 1.5986\n",
      "Epoch 50/50\n",
      "184/184 [==============================] - 1s 5ms/step - d_loss: 0.3907 - g_loss: 1.6051\n",
      "(6139, 20)\n",
      "Epoch 1/50\n",
      "192/192 [==============================] - 2s 5ms/step - d_loss: 0.6938 - g_loss: 0.7835\n",
      "Epoch 2/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.6736 - g_loss: 0.7962\n",
      "Epoch 3/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.6662 - g_loss: 0.7900\n",
      "Epoch 4/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.6534 - g_loss: 0.8116\n",
      "Epoch 5/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.6395 - g_loss: 0.8354\n",
      "Epoch 6/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.6232 - g_loss: 0.8583\n",
      "Epoch 7/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.6138 - g_loss: 0.8818\n",
      "Epoch 8/50\n",
      "192/192 [==============================] - 1s 6ms/step - d_loss: 0.6014 - g_loss: 0.9105\n",
      "Epoch 9/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.5908 - g_loss: 0.9399\n",
      "Epoch 10/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.5809 - g_loss: 0.9666\n",
      "Epoch 11/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.5698 - g_loss: 0.9855\n",
      "Epoch 12/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.5611 - g_loss: 1.0195\n",
      "Epoch 13/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.5538 - g_loss: 1.0309\n",
      "Epoch 14/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.5449 - g_loss: 1.0612\n",
      "Epoch 15/50\n",
      "192/192 [==============================] - 1s 6ms/step - d_loss: 0.5352 - g_loss: 1.0901: 0s - d_loss:\n",
      "Epoch 16/50\n",
      "192/192 [==============================] - 1s 6ms/step - d_loss: 0.5296 - g_loss: 1.1159\n",
      "Epoch 17/50\n",
      "192/192 [==============================] - 1s 6ms/step - d_loss: 0.5242 - g_loss: 1.1208\n",
      "Epoch 18/50\n",
      "192/192 [==============================] - 1s 6ms/step - d_loss: 0.5152 - g_loss: 1.1350\n",
      "Epoch 19/50\n",
      "192/192 [==============================] - 1s 6ms/step - d_loss: 0.5142 - g_loss: 1.1601\n",
      "Epoch 20/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.5067 - g_loss: 1.1693\n",
      "Epoch 21/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.5057 - g_loss: 1.1770\n",
      "Epoch 22/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4999 - g_loss: 1.2061\n",
      "Epoch 23/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4954 - g_loss: 1.2013\n",
      "Epoch 24/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4937 - g_loss: 1.2169\n",
      "Epoch 25/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4937 - g_loss: 1.2408\n",
      "Epoch 26/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4863 - g_loss: 1.2371\n",
      "Epoch 27/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4869 - g_loss: 1.2541\n",
      "Epoch 28/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4837 - g_loss: 1.2389\n",
      "Epoch 29/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4771 - g_loss: 1.2682\n",
      "Epoch 30/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4784 - g_loss: 1.2743\n",
      "Epoch 31/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4738 - g_loss: 1.2781\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 1s 6ms/step - d_loss: 0.4761 - g_loss: 1.2898\n",
      "Epoch 33/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4680 - g_loss: 1.3115\n",
      "Epoch 34/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4648 - g_loss: 1.2964\n",
      "Epoch 35/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4669 - g_loss: 1.3213\n",
      "Epoch 36/50\n",
      "192/192 [==============================] - 1s 6ms/step - d_loss: 0.4620 - g_loss: 1.3319\n",
      "Epoch 37/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4575 - g_loss: 1.3327\n",
      "Epoch 38/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4540 - g_loss: 1.3479\n",
      "Epoch 39/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4539 - g_loss: 1.3458\n",
      "Epoch 40/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4538 - g_loss: 1.3570\n",
      "Epoch 41/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4486 - g_loss: 1.3659\n",
      "Epoch 42/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4497 - g_loss: 1.3619\n",
      "Epoch 43/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4487 - g_loss: 1.3734\n",
      "Epoch 44/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4463 - g_loss: 1.3912\n",
      "Epoch 45/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4397 - g_loss: 1.4001\n",
      "Epoch 46/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4446 - g_loss: 1.4026\n",
      "Epoch 47/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4414 - g_loss: 1.3953\n",
      "Epoch 48/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4371 - g_loss: 1.4068\n",
      "Epoch 49/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4373 - g_loss: 1.4316\n",
      "Epoch 50/50\n",
      "192/192 [==============================] - 1s 5ms/step - d_loss: 0.4332 - g_loss: 1.4152\n"
     ]
    }
   ],
   "source": [
    "# koşullar:\n",
    "list_condition = list(np.unique(y_train))\n",
    "\n",
    "generated_df = pd.DataFrame(columns=data.columns[:])\n",
    "X_train = np.float32(X_train)\n",
    "conditional_datasets = {}\n",
    "\n",
    "for cond in list_condition:\n",
    "    indices = np.where(y_train == cond)[0]\n",
    "    train_data_cond = []\n",
    "    for i in range(len(y_train)):\n",
    "        if i in indices:\n",
    "            train_data_cond.append(X_train[i])\n",
    "    train_data_cond = np.array(train_data_cond)\n",
    "    print(train_data_cond.shape)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_data_cond).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    conditional_datasets[cond] = train_dataset\n",
    "    \n",
    "    discriminator = Discriminator(out_shape=out_shape)\n",
    "    generator = Generator(out_shape=out_shape)\n",
    "    gan = GAN(discriminator, generator, latent_dim)\n",
    "    gan.compile(discriminator_optimizer, generator_optimizer, loss_fn)\n",
    "    gan.fit(conditional_datasets[cond], epochs=50)\n",
    "    \n",
    "    num_gen = len(indices)\n",
    "    random_latent_vectors = tf.random.normal(shape=(num_gen, latent_dim))\n",
    "    generated_data = generator(random_latent_vectors)\n",
    "    gen = pd.DataFrame(np.array(generated_data), columns=data.columns.drop(condition_feature))\n",
    "    gen[condition_feature] = cond\n",
    "    generated_df = pd.concat([generated_df, gen], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_df.to_csv(\"C:/Users/kalybeai-dxlc693/Desktop/GANS/modular-conditional-gan-main/datasets/output_synt/adult.csv\",\n",
    "#               index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KALYBE~1\\AppData\\Local\\Temp/ipykernel_8308/2010494955.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  gen_features = scaler.inverse_transform(generated_df.drop(condition_feature, 1))\n"
     ]
    }
   ],
   "source": [
    "gen_features = scaler.inverse_transform(generated_df.drop(condition_feature, 1))\n",
    "gen_df = pd.DataFrame(gen_features, columns=data.columns.drop(condition_feature))\n",
    "gen_df[condition_feature] = generated_df.loc[:, condition_feature].values\n",
    "gen_df.to_csv(\"C:/Users/kalybeai-dxlc693/Desktop/GANS/modular-conditional-gan-main/datasets/output_synt/{0}\".format(file_name),\n",
    "              index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CROSSING</th>\n",
       "      <th>FINISSHING</th>\n",
       "      <th>HEADING_ACCURACY</th>\n",
       "      <th>SHORT_PASSING</th>\n",
       "      <th>VOLLEYS</th>\n",
       "      <th>DRIBBLING</th>\n",
       "      <th>CURVE</th>\n",
       "      <th>FREE_KICK_ACCURACY</th>\n",
       "      <th>LONG_PASSING</th>\n",
       "      <th>BALL_CONTROL</th>\n",
       "      <th>...</th>\n",
       "      <th>SPRINT_SPEED</th>\n",
       "      <th>AGILITY</th>\n",
       "      <th>REACTION</th>\n",
       "      <th>BALANCE</th>\n",
       "      <th>SHOT_POWER</th>\n",
       "      <th>JUMPING</th>\n",
       "      <th>STAMINA</th>\n",
       "      <th>STRENGTH</th>\n",
       "      <th>LONG_SHOTS</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.369198</td>\n",
       "      <td>53.038166</td>\n",
       "      <td>67.785332</td>\n",
       "      <td>65.801987</td>\n",
       "      <td>62.717884</td>\n",
       "      <td>58.477013</td>\n",
       "      <td>49.674732</td>\n",
       "      <td>47.460831</td>\n",
       "      <td>60.358841</td>\n",
       "      <td>60.419006</td>\n",
       "      <td>...</td>\n",
       "      <td>59.147877</td>\n",
       "      <td>61.022701</td>\n",
       "      <td>64.978867</td>\n",
       "      <td>59.573414</td>\n",
       "      <td>67.071388</td>\n",
       "      <td>76.026398</td>\n",
       "      <td>78.978416</td>\n",
       "      <td>74.700813</td>\n",
       "      <td>57.496456</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.990524</td>\n",
       "      <td>61.410789</td>\n",
       "      <td>54.064697</td>\n",
       "      <td>62.089520</td>\n",
       "      <td>44.034294</td>\n",
       "      <td>54.034668</td>\n",
       "      <td>53.334137</td>\n",
       "      <td>38.564259</td>\n",
       "      <td>54.959576</td>\n",
       "      <td>58.579166</td>\n",
       "      <td>...</td>\n",
       "      <td>73.454659</td>\n",
       "      <td>54.058746</td>\n",
       "      <td>61.665184</td>\n",
       "      <td>53.327736</td>\n",
       "      <td>69.724762</td>\n",
       "      <td>57.965775</td>\n",
       "      <td>80.399536</td>\n",
       "      <td>80.388718</td>\n",
       "      <td>65.665611</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56.857319</td>\n",
       "      <td>53.314262</td>\n",
       "      <td>70.696747</td>\n",
       "      <td>73.294975</td>\n",
       "      <td>64.709000</td>\n",
       "      <td>59.477791</td>\n",
       "      <td>49.803329</td>\n",
       "      <td>65.792801</td>\n",
       "      <td>65.506424</td>\n",
       "      <td>64.538033</td>\n",
       "      <td>...</td>\n",
       "      <td>67.192078</td>\n",
       "      <td>77.428825</td>\n",
       "      <td>62.577820</td>\n",
       "      <td>70.020142</td>\n",
       "      <td>74.606041</td>\n",
       "      <td>75.185661</td>\n",
       "      <td>81.847977</td>\n",
       "      <td>58.529724</td>\n",
       "      <td>69.007797</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.712204</td>\n",
       "      <td>69.368919</td>\n",
       "      <td>74.999847</td>\n",
       "      <td>58.860401</td>\n",
       "      <td>58.536587</td>\n",
       "      <td>57.842995</td>\n",
       "      <td>49.046814</td>\n",
       "      <td>37.339455</td>\n",
       "      <td>45.302471</td>\n",
       "      <td>69.514946</td>\n",
       "      <td>...</td>\n",
       "      <td>64.701317</td>\n",
       "      <td>61.186234</td>\n",
       "      <td>60.054466</td>\n",
       "      <td>70.494019</td>\n",
       "      <td>76.805496</td>\n",
       "      <td>72.774109</td>\n",
       "      <td>75.325188</td>\n",
       "      <td>74.307831</td>\n",
       "      <td>66.370018</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.932060</td>\n",
       "      <td>68.526047</td>\n",
       "      <td>63.752682</td>\n",
       "      <td>66.746307</td>\n",
       "      <td>50.448475</td>\n",
       "      <td>51.851318</td>\n",
       "      <td>43.077579</td>\n",
       "      <td>62.629627</td>\n",
       "      <td>45.311783</td>\n",
       "      <td>72.424850</td>\n",
       "      <td>...</td>\n",
       "      <td>59.835274</td>\n",
       "      <td>57.269634</td>\n",
       "      <td>65.995094</td>\n",
       "      <td>68.093445</td>\n",
       "      <td>77.841347</td>\n",
       "      <td>73.610924</td>\n",
       "      <td>76.804642</td>\n",
       "      <td>80.353645</td>\n",
       "      <td>67.554886</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>65.041573</td>\n",
       "      <td>54.693844</td>\n",
       "      <td>57.667202</td>\n",
       "      <td>68.633949</td>\n",
       "      <td>43.400078</td>\n",
       "      <td>75.780083</td>\n",
       "      <td>70.655518</td>\n",
       "      <td>51.999325</td>\n",
       "      <td>72.449669</td>\n",
       "      <td>74.508553</td>\n",
       "      <td>...</td>\n",
       "      <td>65.225441</td>\n",
       "      <td>70.613846</td>\n",
       "      <td>60.053474</td>\n",
       "      <td>68.865524</td>\n",
       "      <td>64.439827</td>\n",
       "      <td>67.473267</td>\n",
       "      <td>59.581757</td>\n",
       "      <td>75.317574</td>\n",
       "      <td>72.676033</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>64.390335</td>\n",
       "      <td>61.917274</td>\n",
       "      <td>64.056923</td>\n",
       "      <td>67.948135</td>\n",
       "      <td>44.459702</td>\n",
       "      <td>70.798767</td>\n",
       "      <td>53.474060</td>\n",
       "      <td>68.557487</td>\n",
       "      <td>65.607994</td>\n",
       "      <td>70.640816</td>\n",
       "      <td>...</td>\n",
       "      <td>72.978882</td>\n",
       "      <td>62.563862</td>\n",
       "      <td>67.489647</td>\n",
       "      <td>57.104622</td>\n",
       "      <td>72.585083</td>\n",
       "      <td>59.688934</td>\n",
       "      <td>68.180122</td>\n",
       "      <td>76.652412</td>\n",
       "      <td>64.440277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>65.579224</td>\n",
       "      <td>29.390810</td>\n",
       "      <td>68.536766</td>\n",
       "      <td>67.846573</td>\n",
       "      <td>30.058270</td>\n",
       "      <td>56.990070</td>\n",
       "      <td>52.536694</td>\n",
       "      <td>39.267315</td>\n",
       "      <td>71.641106</td>\n",
       "      <td>57.124081</td>\n",
       "      <td>...</td>\n",
       "      <td>72.419411</td>\n",
       "      <td>53.638695</td>\n",
       "      <td>60.053474</td>\n",
       "      <td>55.020351</td>\n",
       "      <td>46.016037</td>\n",
       "      <td>57.898792</td>\n",
       "      <td>67.067268</td>\n",
       "      <td>68.711716</td>\n",
       "      <td>37.239124</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>65.472084</td>\n",
       "      <td>56.768116</td>\n",
       "      <td>62.455875</td>\n",
       "      <td>69.033340</td>\n",
       "      <td>47.014202</td>\n",
       "      <td>66.850983</td>\n",
       "      <td>62.481159</td>\n",
       "      <td>58.368164</td>\n",
       "      <td>65.249588</td>\n",
       "      <td>68.740540</td>\n",
       "      <td>...</td>\n",
       "      <td>70.762413</td>\n",
       "      <td>71.989563</td>\n",
       "      <td>62.712570</td>\n",
       "      <td>69.828949</td>\n",
       "      <td>64.751091</td>\n",
       "      <td>66.275459</td>\n",
       "      <td>74.377098</td>\n",
       "      <td>66.075325</td>\n",
       "      <td>66.119301</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>63.666016</td>\n",
       "      <td>54.719219</td>\n",
       "      <td>58.364998</td>\n",
       "      <td>67.418907</td>\n",
       "      <td>41.558041</td>\n",
       "      <td>64.398872</td>\n",
       "      <td>54.109932</td>\n",
       "      <td>54.400898</td>\n",
       "      <td>62.257874</td>\n",
       "      <td>62.954483</td>\n",
       "      <td>...</td>\n",
       "      <td>72.525978</td>\n",
       "      <td>67.716576</td>\n",
       "      <td>60.388046</td>\n",
       "      <td>71.449142</td>\n",
       "      <td>60.355370</td>\n",
       "      <td>65.842567</td>\n",
       "      <td>68.284203</td>\n",
       "      <td>66.229538</td>\n",
       "      <td>61.325459</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CROSSING  FINISSHING  HEADING_ACCURACY  SHORT_PASSING    VOLLEYS  \\\n",
       "0      57.369198   53.038166         67.785332      65.801987  62.717884   \n",
       "1      50.990524   61.410789         54.064697      62.089520  44.034294   \n",
       "2      56.857319   53.314262         70.696747      73.294975  64.709000   \n",
       "3      47.712204   69.368919         74.999847      58.860401  58.536587   \n",
       "4      38.932060   68.526047         63.752682      66.746307  50.448475   \n",
       "...          ...         ...               ...            ...        ...   \n",
       "11995  65.041573   54.693844         57.667202      68.633949  43.400078   \n",
       "11996  64.390335   61.917274         64.056923      67.948135  44.459702   \n",
       "11997  65.579224   29.390810         68.536766      67.846573  30.058270   \n",
       "11998  65.472084   56.768116         62.455875      69.033340  47.014202   \n",
       "11999  63.666016   54.719219         58.364998      67.418907  41.558041   \n",
       "\n",
       "       DRIBBLING      CURVE  FREE_KICK_ACCURACY  LONG_PASSING  BALL_CONTROL  \\\n",
       "0      58.477013  49.674732           47.460831     60.358841     60.419006   \n",
       "1      54.034668  53.334137           38.564259     54.959576     58.579166   \n",
       "2      59.477791  49.803329           65.792801     65.506424     64.538033   \n",
       "3      57.842995  49.046814           37.339455     45.302471     69.514946   \n",
       "4      51.851318  43.077579           62.629627     45.311783     72.424850   \n",
       "...          ...        ...                 ...           ...           ...   \n",
       "11995  75.780083  70.655518           51.999325     72.449669     74.508553   \n",
       "11996  70.798767  53.474060           68.557487     65.607994     70.640816   \n",
       "11997  56.990070  52.536694           39.267315     71.641106     57.124081   \n",
       "11998  66.850983  62.481159           58.368164     65.249588     68.740540   \n",
       "11999  64.398872  54.109932           54.400898     62.257874     62.954483   \n",
       "\n",
       "       ...  SPRINT_SPEED    AGILITY   REACTION    BALANCE  SHOT_POWER  \\\n",
       "0      ...     59.147877  61.022701  64.978867  59.573414   67.071388   \n",
       "1      ...     73.454659  54.058746  61.665184  53.327736   69.724762   \n",
       "2      ...     67.192078  77.428825  62.577820  70.020142   74.606041   \n",
       "3      ...     64.701317  61.186234  60.054466  70.494019   76.805496   \n",
       "4      ...     59.835274  57.269634  65.995094  68.093445   77.841347   \n",
       "...    ...           ...        ...        ...        ...         ...   \n",
       "11995  ...     65.225441  70.613846  60.053474  68.865524   64.439827   \n",
       "11996  ...     72.978882  62.563862  67.489647  57.104622   72.585083   \n",
       "11997  ...     72.419411  53.638695  60.053474  55.020351   46.016037   \n",
       "11998  ...     70.762413  71.989563  62.712570  69.828949   64.751091   \n",
       "11999  ...     72.525978  67.716576  60.388046  71.449142   60.355370   \n",
       "\n",
       "         JUMPING    STAMINA   STRENGTH  LONG_SHOTS    y  \n",
       "0      76.026398  78.978416  74.700813   57.496456  0.0  \n",
       "1      57.965775  80.399536  80.388718   65.665611  0.0  \n",
       "2      75.185661  81.847977  58.529724   69.007797  0.0  \n",
       "3      72.774109  75.325188  74.307831   66.370018  0.0  \n",
       "4      73.610924  76.804642  80.353645   67.554886  0.0  \n",
       "...          ...        ...        ...         ...  ...  \n",
       "11995  67.473267  59.581757  75.317574   72.676033  1.0  \n",
       "11996  59.688934  68.180122  76.652412   64.440277  1.0  \n",
       "11997  57.898792  67.067268  68.711716   37.239124  1.0  \n",
       "11998  66.275459  74.377098  66.075325   66.119301  1.0  \n",
       "11999  65.842567  68.284203  66.229538   61.325459  1.0  \n",
       "\n",
       "[12000 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dsafasafafdfdsfsdafsd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KALYBE~1\\AppData\\Local\\Temp/ipykernel_8308/2167412725.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdsafasafafdfdsfsdafsd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dsafasafafdfdsfsdafsd' is not defined"
     ]
    }
   ],
   "source": [
    "dsafasafafdfdsfsdafsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu repo görsel üreten bir gan modelini ele alır (vanilla)\n",
    "Bu repo üzerinde 2 aşamada conditional tabular data için veri üretiyor olacağız\n",
    "* Koşullu yapma\n",
    "* Tabular datada çalışıyor hale getirme\n",
    "\n",
    "Şu anda 2. aşamada çalışılınıyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path \n",
    "\n",
    "import PIL \n",
    "import imageio\n",
    "from IPython import display\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import sys \n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from gan.networks import Generator, Discriminator, GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"./gan/datasets/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_image, train_labels), (_, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_image.reshape(train_image.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 32\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# belirli bir koşul için veri seti hazırlama (koşul \"= 5\" olması)\n",
    "indices = np.where(train_labels == 5)[0]\n",
    "train_images_5 = []\n",
    "for i in range(len(train_labels)):\n",
    "    if i in indices:\n",
    "        train_images_5.append(train_images[i])\n",
    "        \n",
    "train_images_5 = np.array(train_images_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images_5).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toplamda 60.000 görsel var\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toplamda 5421 adet \"5\" görseli var\n",
    "train_images_5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki model oluşturma adımında hangi koşul yazılırsa yazılsın model gerçek veya sahte ayrımı yapmaya çalışacaktır.\n",
    "Bu yüzden önemli nokta veri setini ayrıştırıp ona vermektir örneğin sadece 5 için görsel çizdirmeye çalışalım."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(labels, output):\n",
    "    return keras.losses.BinaryCrossentropy(from_logits=True)(labels, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.05)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "gan = GAN(discriminator, generator, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(discriminator_optimizer, generator_optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = generator(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = discriminator(generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deney\n",
    "# ilk deney koşula uygun n adet yani birden fazla sentetik veri üretebilir miyiz?\n",
    "# ikinci deney farklı koşullar için tekrar ve tekrar model eğitmek ile tek bir model eğitmek arasında ne gibi maliyet farkı var?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deney 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "num_images = 49\n",
    "random_latent_vectors = tf.random.normal(shape=(num_images, latent_dim))\n",
    "generated_images = generator(random_latent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(generated_images[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deney 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# vanilla gan tüm veriler ile eğitiliyor.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "gan = GAN(discriminator, generator, latent_dim)\n",
    "gan.compile(discriminator_optimizer, generator_optimizer, loss_fn)\n",
    "gan.fit(train_dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32 dk 46 saniye - 60000x28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for spec in range(10):\n",
    "    indices = np.where(train_labels == spec)[0]\n",
    "    train_images_spec = []\n",
    "    for i in range(len(train_labels)):\n",
    "        if i in indices:\n",
    "            train_images_spec.append(train_images[i])\n",
    "\n",
    "    train_images_spec = np.array(train_images_spec)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_images_spec).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    print(\"for spec:\", spec)\n",
    "    print(\"len train_images_spec:\", len(train_images_spec))\n",
    "    \n",
    "    discriminator = Discriminator()\n",
    "    generator = Generator()\n",
    "    gan = GAN(discriminator, generator, latent_dim)\n",
    "    gan.compile(discriminator_optimizer, generator_optimizer, loss_fn)\n",
    "    gan.fit(train_dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33 dk - her veri ayrı ayrı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aşarısı sadece modellerin çıktısını test etmek için"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "list_img = []\n",
    "for spec in range(10):\n",
    "    indices = np.where(train_labels == spec)[0]\n",
    "    train_images_spec = []\n",
    "    for i in range(len(train_labels)):\n",
    "        if i in indices:\n",
    "            train_images_spec.append(train_images[i])\n",
    "\n",
    "    train_images_spec = np.array(train_images_spec)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_images_spec).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    print(\"spec:\", spec)\n",
    "    print(\"len train_images_spec:\", len(train_images_spec))\n",
    "    print(\"ilk on:\", indices[:10])\n",
    "    \n",
    "    discriminator = Discriminator()\n",
    "    generator = Generator()\n",
    "    gan = GAN(discriminator, generator, latent_dim)\n",
    "    gan.compile(discriminator_optimizer, generator_optimizer, loss_fn)\n",
    "    gan.fit(train_dataset, epochs=20)\n",
    "    noise = tf.random.normal([1, 100])\n",
    "    generated_image = generator(noise)\n",
    "    list_img.append(generated_image[0, :, :, 0])\n",
    "    #plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.imshow(list_img[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not: Şu anda ilk aşama tamamlandı. Yani görsel veriler için belirli bir koşula uygun veri üretimi vanilla gan kullanılarak gerçekleştirildi.\n",
    "\n",
    "Birden fazla örnek oluşturma veya oluşturulan örneklerin gösterilmesi ile ilgili bir problem gözlemlenmedi.\n",
    "\n",
    "Ayrıca modellerin tek tek eğitilmesi veya tüm verilerin aynı anda koşuldan bağımsız eğitilmesi konusunda bir zaman farkı bulunmamaktadır.\n",
    "\n",
    "Bizim verilerimizin çok fazla boyuta sahip olduğu için bu kadar uzun sürdü eğitimler örneğin, adult veri seti 32.500x15 boyutundadır, bizim görsel veri setimiz 60.000x48x48 yani yaklaşık 96 katı boyutunda.\n",
    "\n",
    "Burada bir sonraki aşama tabular data için eğitim gerçekleştirebilmektir.\n",
    "Daha sonrasında işlemlerin fonksiyonlaştırılması ve nesneye yönelimli programlama yapısında py dosyası formatına getirilmesidir.\n",
    "\n",
    "En sonunda da bu modelin eğitimi hızlandırılmaya çalışılabilir. Veriyi temsil eden en iyi bir örneklem seçilebilir vs. veya paralelleştirme denenebilir. Modelin içine koşul verilmeye çalışılabilir. Early stopping eklenebilir. Veri boyutuna bağlı epoch sayısı belirlenebilir. Fakat version 1 için elde çalışan bir yapının bulunması iyi olacaktır. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şu anda 2. aşamadayız yani görsel verisi için eğittiğimiz modeli tabular data için eğiteceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path \n",
    "\n",
    "import PIL \n",
    "import imageio\n",
    "from IPython import display\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import sys \n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from gan.networks import Generator, Discriminator, GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/kalybeai-dxlc693/Desktop/GANS/adult.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "for i in ['workclass','education','marital.status','occupation','relationship','race','sex','native.country','income']:\n",
    "    df[i] = le.fit_transform(df[i].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(df.drop('income', 1))\n",
    "y_train = df['income'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 32\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(labels, output):\n",
    "    return keras.losses.BinaryCrossentropy(from_logits=True)(labels, output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.05)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.float32(X_train)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# vanilla gan tüm veriler ile eğitiliyor.\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "gan = GAN(discriminator, generator, latent_dim)\n",
    "gan.compile(discriminator_optimizer, generator_optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan.fit(train_dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = generator(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(generated_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# önce train içindeki kısım incelenecek\n",
    "# sonra koşula uygun yapılmaya çalışılacaktır\n",
    "# class sayısı dinamik yapılacak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generated_df = pd.DataFrame(columns=data.columns[:])\n",
    "X_train = np.float32(X_train)\n",
    "conditional_datasets = []\n",
    "list_condition = [0, 1]\n",
    "for cond in list_condition:\n",
    "    indices = np.where(y_train == cond)[0]\n",
    "    train_data_cond = []\n",
    "    for i in range(len(y_train)):\n",
    "        if i in indices:\n",
    "            train_data_cond.append(X_train[i])\n",
    "    train_data_cond = np.array(train_data_cond)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    conditional_datasets.append(train_dataset)\n",
    "    \n",
    "    discriminator = Discriminator()\n",
    "    generator = Generator()\n",
    "    gan = GAN(discriminator, generator, latent_dim)\n",
    "    gan.compile(discriminator_optimizer, generator_optimizer, loss_fn)\n",
    "    gan.fit(conditional_datasets[cond], epochs=50)\n",
    "    \n",
    "    num_gen = len(indices)\n",
    "    random_latent_vectors = tf.random.normal(shape=(num_gen, latent_dim))\n",
    "    generated_data = generator(random_latent_vectors)\n",
    "    gen = pd.DataFrame(np.array(generated_data), columns=data.columns[:-1])\n",
    "    gen[data.columns[-1]] = cond\n",
    "    generated_df = pd.concat([generated_df, gen], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df.to_csv(\"C:/Users/kalybeai-dxlc693/Desktop/GANS/modular-conditional-gan-main/datasets/output_synt/adult.csv\",\n",
    "              index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_features = scaler.inverse_transform(generated_df.drop('income', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_df = pd.DataFrame(gen_features, columns=data.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_df[data.columns[-1]] = generated_df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_df.to_csv(\"C:/Users/kalybeai-dxlc693/Desktop/GANS/modular-conditional-gan-main/datasets/output_synt/adult.csv\",\n",
    "              index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conditional_datasets[0]\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "gan = GAN(discriminator, generator, latent_dim)\n",
    "gan.compile(discriminator_optimizer, generator_optimizer, loss_fn)\n",
    "gan.fit(conditional_datasets[0], epochs=10)\n",
    "num_gen = list_count[cond]\n",
    "random_latent_vectors = tf.random.normal(shape=(num_gen, latent_dim))\n",
    "generated_data = generator(random_latent_vectors)\n",
    "gen0 = pd.DataFrame(np.array(generated_data), columns=data.columns[:-1])\n",
    "gen0[data.columns[-1]] = cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conditional_datasets[0]\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "gan = GAN(discriminator, generator, latent_dim)\n",
    "gan.compile(discriminator_optimizer, generator_optimizer, loss_fn)\n",
    "gan.fit(conditional_datasets[1], epochs=10)\n",
    "num_gen = list_count[cond]\n",
    "random_latent_vectors = tf.random.normal(shape=(num_gen, latent_dim))\n",
    "generated_data = generator(random_latent_vectors)\n",
    "gen1 = pd.DataFrame(np.array(generated_data), columns=data.columns[:-1])\n",
    "gen1[data.columns[-1]] = cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gen = list_count[cond]\n",
    "random_latent_vectors = tf.random.normal(shape=(num_gen, latent_dim))\n",
    "generated_data = generator(random_latent_vectors)\n",
    "gen0 = pd.DataFrame(np.array(generated_data), columns=data.columns[:-1])\n",
    "gen0[data.columns[-1]] = cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen0 = pd.DataFrame(np.array(generated_data), columns=data.columns[:-1])\n",
    "gen0[data.columns[-1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular Data için Çalışan Koşullu GAN'ın Dinamik hale getirilmesi ve Fonksiyonelleştirilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import PIL \n",
    "import imageio\n",
    "from IPython import display\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import sys \n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from gan.networks import Generator, Discriminator, GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/kalybeai-dxlc693/Desktop/GANS/adult.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "df = data.copy()\n",
    "\n",
    "# preprocess\n",
    "le = preprocessing.LabelEncoder()\n",
    "for i in ['workclass','education','marital.status','occupation','relationship','race','sex','native.country','income']:\n",
    "    df[i] = le.fit_transform(df[i].astype(str))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(df.drop('income', 1))\n",
    "y_train = df['income'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 32\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss & optimizer\n",
    "def loss_fn(labels, output):\n",
    "    return keras.losses.BinaryCrossentropy(from_logits=True)(labels, output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.05)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# koşullar:\n",
    "list_condition = list(np.unique(y_train))\n",
    "condition_feature = \"income\"\n",
    "\n",
    "generated_df = pd.DataFrame(columns=data.columns[:])\n",
    "X_train = np.float32(X_train)\n",
    "conditional_datasets = {}\n",
    "\n",
    "for cond in list_condition:\n",
    "    indices = np.where(y_train == cond)[0]\n",
    "    train_data_cond = []\n",
    "    for i in range(len(y_train)):\n",
    "        if i in indices:\n",
    "            train_data_cond.append(X_train[i])\n",
    "    train_data_cond = np.array(train_data_cond)\n",
    "    print(train_data_cond.shape)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_data_cond).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    conditional_datasets[cond] = train_dataset\n",
    "    \n",
    "    discriminator = Discriminator()\n",
    "    generator = Generator()\n",
    "    gan = GAN(discriminator, generator, latent_dim)\n",
    "    gan.compile(discriminator_optimizer, generator_optimizer, loss_fn)\n",
    "    gan.fit(conditional_datasets[cond], epochs=50)\n",
    "    \n",
    "    num_gen = len(indices)\n",
    "    random_latent_vectors = tf.random.normal(shape=(num_gen, latent_dim))\n",
    "    generated_data = generator(random_latent_vectors)\n",
    "    gen = pd.DataFrame(np.array(generated_data), columns=data.columns.drop(condition_feature))\n",
    "    gen[condition_feature] = cond\n",
    "    generated_df = pd.concat([generated_df, gen], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# koşullar:\n",
    "list_condition = [0, 1]\n",
    "\n",
    "generated_df = pd.DataFrame(columns=data.columns[:])\n",
    "X_train = np.float32(X_train)\n",
    "conditional_datasets = {}\n",
    "\n",
    "\n",
    "for cond in list_condition:\n",
    "    indices = np.where(y_train == cond)[0]\n",
    "    train_data_cond = []\n",
    "    for i in range(len(y_train)):\n",
    "        if i in indices:\n",
    "            train_data_cond.append(X_train[i])\n",
    "    train_data_cond = np.array(train_data_cond)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    conditional_datasets[cond] = train_dataset\n",
    "    \n",
    "    discriminator = Discriminator()\n",
    "    generator = Generator()\n",
    "    gan = GAN(discriminator, generator, latent_dim)\n",
    "    gan.compile(discriminator_optimizer, generator_optimizer, loss_fn)\n",
    "    gan.fit(conditional_datasets[cond], epochs=50)\n",
    "    \n",
    "    num_gen = len(indices)\n",
    "    random_latent_vectors = tf.random.normal(shape=(num_gen, latent_dim))\n",
    "    generated_data = generator(random_latent_vectors)\n",
    "    gen = pd.DataFrame(np.array(generated_data), columns=data.columns[:-1])\n",
    "    gen[data.columns[-1]] = cond\n",
    "    generated_df = pd.concat([generated_df, gen], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# koşullar:\n",
    "list_condition = list(np.unique(y_train))\n",
    "condition_feature = \"income\"\n",
    "\n",
    "generated_df = pd.DataFrame(columns=data.columns[:])\n",
    "X_train = np.float32(X_train)\n",
    "conditional_datasets = {}\n",
    "\n",
    "for cond in list_condition:\n",
    "    indices = np.where(y_train == cond)[0]\n",
    "    train_data_cond = []\n",
    "    for i in range(len(y_train)):\n",
    "        if i in indices:\n",
    "            train_data_cond.append(X_train[i])\n",
    "    train_data_cond = np.array(train_data_cond)\n",
    "    print(train_data_cond.shape)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_data_cond).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    conditional_datasets[cond] = train_dataset\n",
    "    \n",
    "    discriminator = Discriminator()\n",
    "    generator = Generator()\n",
    "    gan = GAN(discriminator, generator, latent_dim)\n",
    "    gan.compile(discriminator_optimizer, generator_optimizer, loss_fn)\n",
    "    gan.fit(conditional_datasets[cond], epochs=50)\n",
    "    \n",
    "    num_gen = len(indices)\n",
    "    random_latent_vectors = tf.random.normal(shape=(num_gen, latent_dim))\n",
    "    generated_data = generator(random_latent_vectors)\n",
    "    gen = pd.DataFrame(np.array(generated_data), columns=data.columns.drop(condition_feature))\n",
    "    gen[condition_feature] = cond\n",
    "    generated_df = pd.concat([generated_df, gen], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df[\"income\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
